#!/usr/bin/env
"""Pipeline utilities"""
import os
import logging
import gem
import traceback
import json
import multiprocessing as mp

from gem.utils import Timer
import gem.gemtools as gt

class dotdict(dict):
    def __getattr__(self, attr):
        return self.get(attr, None)
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


class PipelineError(Exception):
    """Exception thrown by the mapping pipeline"""
    def __init__(self, message):
        self.message = message


class PipelineStep(object):
    """General mapping pipeline step"""
    def __init__(self, name, dependencies=None, final=False, description=""):
        self.id = None
        self.name = name
        self.description = description
        self.pipeline = None
        self.dependencies = []
        self._files = None
        self.configuration = None
        self.final = final
        if dependencies is not None:
            self.dependencies.extend(dependencies)

    def prepare(self, id, pipeline, configuration):
        """Implement this to prepare the step"""
        self.pipeline = pipeline
        self.id = id
        self.configuration = configuration
        # initialize files
        self.files()

    def run(self, final=False):
        """Implement this method to execute the step"""
        pass

    def cleanup(self, force=False):
        if force or (not self.final and self.pipeline.remove_temp):
            for f in self.files():
                if os.path.exists(f):
                    logging.gemtools.debug("Remove temporary file %s" % f)
                    os.remove(f)

    def files(self):
        """Return the output files generated by this step.
        By default one .map output file is generated
        """
        if self._files is None:
            self._files = []
            self._files.append(self.pipeline.create_file_name(self.name, final=self.final))
        return self._files

    def _compress(self):
        """Returns true if this step needs compression for
        all output
        """
        return self.pipeline.compress_all

    def _output(self):
        """Return the output file if its not final
        step, otherwise return none
        """
        if self.final:
            return None
        else:
            return self._final_output()

    def _final_output(self):
        """Return the last file created by this step"""
        return self.files()[-1]

    def _input(self, raw=False):
        """Return pipeline input if this step
        has no dependencies or the
        output of the last dependency
        """
        if self.dependencies is None or len(self.dependencies) == 0:
            return self.pipeline.open_input()
        return self.pipeline.open_step(self.dependencies[-1], raw=raw)

    def open(self, raw=False):
        """Open the steps output. The default implementation
        opnes the last file"""
        fs = self.files()
        if len(fs) > 0:
            if raw:
                logging.gemtools.debug("Returning raw step %s output : %s", self.name, fs[-1])
                return fs[-1]
            else:
                logging.gemtools.debug("Opening step %s output : %s", self.name, fs[-1])
                return gem.files.open(fs[-1])
        else:
            logging.error("Step does not produce output files! Unable to open output")
            return None

    def is_done(self):
        """Return true if this step is done and
        does not need execution

        The basic implementation checks if all files exists
        """
        for f in self.files():
            if not os.path.exists(f):
                return False
        return True


class PrepareInputStep(PipelineStep):
    """Prepare multiple input files,
    clean ids and write uncompressd file
    """
    def files(self):
        if self._files is None:
            self._files = []
            self._files.append(self.pipeline.create_file_name(self.name, file_suffix="gt.fastq", final=self.final))
        return self._files

    def __write(self):
        outfile = gt.OutputFile(self._final_output(), clean_id=True, append_extra=False)
        infile = self._input()
        infile.write_stream(outfile, write_map=False)
        infile.close()
        outfile.close()

    def run(self):
        """Merge current set of mappings and delete last ones"""
        p = mp.Process(target=PrepareInputStep.__write, args=(self,))
        p.start()
        p.join()



class MergeStep(PipelineStep):
    """Merge up to the current step"""

    def run(self):
        """Merge current set of mappings and delete last ones"""
        same_content = self.configuration.get("same_content", True)
        inputs = self._input()
        master = inputs[0]
        slaves = inputs[1:]
        mapping = gem.merger(master, slaves).merge(self._output(), self.pipeline.threads, same_content=same_content, compress=self._compress())

        if self.final:
            gem.score(mapping, self.configuration["index"], self._final_output(),
                filter=self.pipeline.filter,
                threads=max(2, self.pipeline.threads / 2),
                quality=self.pipeline.quality,
                compress=self.pipeline.compress
            )

    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("You have to specify what to merge!")
        return [self.pipeline.open_step(i) for i in self.dependencies if i >= 0]


class MergeAndPairStep(PipelineStep):
    """Do merging and pairing in one single step"""
    def run(self):
        cfg = self.configuration
        same_content = self.configuration.get("same_content", True)
        inputs = self._input()
        master = inputs[0]
        slaves = inputs[1:]

        mapping = gem.merger(master, slaves).merge(None, threads=self.pipeline.threads, same_content=same_content, compress=False)

        pair_mapping = gem.pairalign(
            mapping,
            cfg["index"],
            self._output(),
          quality_threshold=cfg["quality_threshold"],
          max_decoded_matches=cfg["max_decoded_matches"],
          min_decoded_strata=cfg["min_decoded_strata"],
          min_insert_size=cfg["min_insert_size"],
          max_insert_size=cfg["max_insert_size"],
          max_edit_distance=cfg["max_edit_distance"],
          min_matched_bases=cfg["min_matched_bases"],
          max_extendable_matches=cfg["max_extendable_matches"],
          max_matches_per_extension=cfg["max_matches_per_extension"],
          threads=max(2, self.pipeline.threads / 2),  # self.pipeline.threads,
          quality=self.pipeline.quality,
          compress=self._compress())

        if self.final:
            gem.score(pair_mapping, cfg["index"], self._final_output(),
                filter=self.pipeline.filter,
                threads=self.pipeline.threads,  # max(2, self.pipeline.threads / 2),
                quality=self.pipeline.quality,
                compress=self.pipeline.compress,
                raw=True)

    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("You have to specify what to merge!")
        return [self.pipeline.open_step(i) for i in self.dependencies if i >= 0]


class CreateBamStep(PipelineStep):
    """Create BAM file"""

    def files(self):
        if self._files is None:
            self._files = []
            self._files.append(self.pipeline.create_file_name(self.name, file_suffix="bam", final=self.final))
        return self._files

    def run(self):
        cfg = self.configuration
        sam = gem.gem2sam(self._input(), cfg["index"], threads=self.pipeline.threads, quality=self.pipeline.quality)
        gem.sam2bam(sam, self._final_output(), sorted=cfg["sort"], mapq=cfg["mapq"], threads=self.pipeline.threads, sort_memory=self.pipeline.sort_memory)


class IndexBamStep(PipelineStep):
    """Index BAM file"""

    def files(self):
        if self._files is None:
            self._files = []
            self._files.append(self.pipeline.create_file_name(self.name, file_suffix="bam.bai", final=self.final))
        return self._files

    def run(self):
        #cfg = self.configuration
        gem.bamIndex(self._input(raw=True), self._final_output())


class MapStep(PipelineStep):
    """Mapping step"""
    def run(self):
        cfg = self.configuration
        mapping = gem.mapper(
            self._input(),
            cfg["index"],
            self._output(),
            mismatches=cfg["mismatches"],
            quality_threshold=cfg["quality_threshold"],
            max_decoded_matches=cfg["max_decoded_matches"],
            min_decoded_strata=cfg["min_decoded_strata"],
            min_matched_bases=cfg["min_matched_bases"],
            max_big_indel_length=cfg["max_big_indel_length"],
            max_edit_distance=cfg["max_edit_distance"],
            mismatch_alphabet=cfg["mismatch_alphabet"],
            delta=cfg["strata_after_best"],
            trim=cfg["trim"],
            quality=self.pipeline.quality,
            threads=self.pipeline.threads,
            compress=self._compress()
        )
        if self.final:
            gem.score(mapping, cfg["index"], self._final_output(),
                filter=self.pipeline.filter,
                threads=max(2, self.pipeline.threads / 2),
                quality=self.pipeline.quality,
                compress=self.pipeline.compress
            )


class PairalignStep(PipelineStep):
    """Pairalign"""
    def run(self):
        cfg = self.configuration
        mapping = gem.pairalign(
            self._input(),
            cfg["index"],
            self._output(),
          quality_threshold=cfg["quality_threshold"],
          max_decoded_matches=cfg["max_decoded_matches"],
          min_decoded_strata=cfg["min_decoded_strata"],
          min_insert_size=cfg["min_insert_size"],
          max_insert_size=cfg["max_insert_size"],
          max_edit_distance=cfg["max_edit_distance"],
          min_matched_bases=cfg["min_matched_bases"],
          max_extendable_matches=cfg["max_extendable_matches"],
          max_matches_per_extension=cfg["max_matches_per_extension"],
          threads=self.pipeline.threads,
          quality=self.pipeline.quality,
          compress=self._compress())

        if self.final:
            gem.score(mapping, cfg["index"], self._final_output(),
                filter=self.pipeline.filter,
                threads=max(2, self.pipeline.threads / 2),
                quality=self.pipeline.quality,
                compress=self.pipeline.compress)


class CreateDenovoTranscriptomeStep(PipelineStep):
    """Create denovo transcriptome"""

    def files(self):
        """Return the output files generated by this step.
        By default one .map output file is generated
        """
        if self._files is None:
            self._files = []
            index_denovo_out = self.pipeline.create_file_name(self.name, file_suffix="gem")
            junctions_out = self.pipeline.create_file_name(self.name, file_suffix="junctions")
            denovo_keys = junctions_out + ".keys"
            self._files.append(index_denovo_out)
            self._files.append(junctions_out)
            self._files.append(junctions_out + ".fa")
            self._files.append(denovo_keys)
            self._files.append(index_denovo_out[:-4] + ".log")
            self.index_denovo_out = index_denovo_out
            self.junctions_out = junctions_out
            self.denovo_keys = denovo_keys
        return self._files

    def run(self):
        """Create the denovo transcriptome"""
        cfg = self.configuration
        (gtf_junctions, junctions_gtf_out) = self.pipeline.gtf_junctions()
        denovo_junctions = gem.extract_junctions(
            self._input(),
            cfg["index"],
            mismatches=cfg["mismatches"],
            threads=self.pipeline.threads,
            strata_after_first=0,
            coverage=cfg["coverage"],
            min_split=cfg["min_split_length"],
            max_split=cfg["max_split_length"],
            refinement_step_size=cfg["refinement_step_size"],
            min_split_size=cfg["min_split_size"],
            matches_threshold=cfg["matches_threshold"],
            max_junction_matches=cfg["max_junction_matches"]
        )

        logging.gemtools.gt("Found Denovo Junctions %d with coverage >= %d" % (len(denovo_junctions), cfg["coverage"]))

        filtered_denovo_junctions = set(gem.junctions.filter_by_distance(denovo_junctions, cfg["min_split_length"], cfg["max_split_length"]))
        logging.gemtools.gt("Denovo junction passing distance filter (min: %d max: %d): %d (%d removed)" % (cfg["min_split_length"], cfg["max_split_length"],
            len(filtered_denovo_junctions), (len(denovo_junctions) - len(filtered_denovo_junctions))))

        if gtf_junctions is not None:
            logging.gemtools.gt("Joining with Annotation - denovo: %d annotation: %d" % (len(filtered_denovo_junctions), len(gtf_junctions)))
            junctions = gtf_junctions.union(filtered_denovo_junctions)
            logging.gemtools.gt("Joined Junctions %d" % (len(junctions)))
            gem.junctions.write_junctions(junctions, self.junctions_out, cfg["index"])
        else:
            logging.gemtools.gt("Skipped mergin with annotation, denovo junctions: %d" % (len(filtered_denovo_junctions)))
            gem.junctions.write_junctions(filtered_denovo_junctions, self.junctions_out, cfg["index"])

        logging.gemtools.gt("Computing denovo transcriptome")
        (denovo_transcriptome, denovo_keys) = gem.compute_transcriptome(self.pipeline.max_read_length, cfg["index"], self.junctions_out, junctions_gtf_out)

        logging.gemtools.gt("Indexing denovo transcriptome")
        gem.index(denovo_transcriptome, self.index_denovo_out, threads=self.pipeline.threads)
        return (self.index_denovo_out, self.denovo_keys)


class TranscriptMapStep(PipelineStep):
    """Transcript Mapping step"""

    def prepare(self, id, pipeline, config):
        PipelineStep.prepare(self, id, pipeline, config)
        if config["denovo"]:
            # denovo transcript mapping
            if config["index"] is None or not os.path.exists(config["index"]):
                # add denovo transript step
                create_step_id = pipeline.create_transcriptome("denovo-index", dependencies=self.dependencies)
                self.dependencies.append(create_step_id)
                self.configuration["create_index"] = create_step_id
                if config["index"] is None:
                    # update the configuration
                    config["index"] = pipeline.steps[create_step_id].index_denovo_out
                    config["keys"] = pipeline.steps[create_step_id].denovo_keys
                # this is ugly but we have to increase the id here as we squeezed in another job
                self.id = create_step_id + 1

    def run(self):
        cfg = self.configuration
        if cfg["denovo"] and cfg["create_index"]:
            step = self.pipeline.steps[cfg["create_index"]]
            cfg["index"] = step.index_denovo_out
            cfg["keys"] = step.denovo_keys

        gem.mapper(
            self._input(),
            cfg["index"],
            self.files()[0],
            mismatches=cfg["mismatches"],
            quality_threshold=cfg["quality_threshold"],
            max_decoded_matches=cfg["max_decoded_matches"],
            min_decoded_strata=cfg["min_decoded_strata"],
            min_matched_bases=cfg["min_matched_bases"],
            max_big_indel_length=cfg["max_big_indel_length"],
            max_edit_distance=cfg["max_edit_distance"],
            mismatch_alphabet=cfg["mismatch_alphabet"],
            delta=cfg["strata_after_best"],
            trim=cfg["trim"],
            key_file=cfg["keys"],
            quality=self.pipeline.quality,
            threads=self.pipeline.threads
        )

    def _input(self, raw=False):
        """Return pipeline input if this step
        has no dependencies or the
        output of the last dependency
        """
        if self.configuration["denovo"]:
            if self.dependencies is None or len(self.dependencies) == 1:
                return self.pipeline.open_input()
            return self.pipeline.open_step(self.dependencies[0], raw=raw)
        else:
            return PipelineStep._input(self, raw=raw)


class MappingPipeline(object):
    """General mapping pipeline class."""

    def __init__(self):
        self.steps = []  # pipeline steps
        self.run_steps = []  # steps to run

        # general parameter
        self.input = None  # input files
        self.name = None  # target name
        self.index = None  # genome index
        self.output_dir = None  # Output directory
        self.annotation = None  # GTF annotation to use
        self.threads = 1  # number of threads
        self.max_read_length = 150  # max read length
        self.transcript_index = None  # transcriptome index
        self.transcript_keys = None  # transcriptome keys file
        self.denovo_index = None  # the denovo index to use
        self.denovo_keys = None  # the denovo keys to use
        self.quality = 33  # quality offset
        self.junctions_file = None  # file with both denovo and GTF junctions
        self.junctions_annotation = None  # file with the annotation junctions
        self.scoring_scheme = "+U,+u,-s,-t,+1,-i,-a"  # scoring scheme
        self.filter = (1, 2, 25)  # result filter
        self.compress = True  # compress final output
        self.compress_all = False  # also compress intermediate output
        self.remove_temp = True  # remove temporary
        self.bam_mapq = 0  # filter bam content mapq
        self.bam_create = True  # create bam
        self.bam_sort = True  # sort bam
        self.bam_index = True  # index bam
        self.single_end = False  # single end alignments
        self.write_config = True  # write configuration
        self.dry = False  # only dry run
        self.sort_memory = "768M"  # samtools sort memory
        self.direct_input = False  # if true, skip the preparation step
        self.force = False  # force computation of all steps

        # genome mapping parameter
        self.genome_mismatches = 0.06
        self.genome_quality_threshold = 26
        self.genome_max_decoded_matches = 20
        self.genome_min_decoded_strata = 2
        self.genome_min_matched_bases = 0.80
        self.genome_max_big_indel_length = 15
        self.genome_max_edit_distance = 0.20
        self.genome_mismatch_alphabet = "ACGT"
        self.genome_strata_after_best = 1

        # transcript mapping parameter
        self.transcript_mismatches = None  # initialize from genom
        self.transcript_quality_threshold = None  # initialize from genome
        self.transcript_max_decoded_matches = 50  # this need to be custom
        self.transcript_min_decoded_strata = None  # initialize from genome ?
        self.transcript_min_matched_bases = None  # initialize from genome  ?
        self.transcript_max_big_indel_length = None  # initialize from genome ?
        self.transcript_max_edit_distance = None  # initialize from genome ?
        self.transcript_mismatch_alphabet = None  # initialize from genome
        self.transcript_strata_after_best = None  # initialize from genome

        # junction detection parameter
        self.junction_mismatches = 0.04
        self.junctions_max_junction_matches = 5
        self.junctions_min_split_length = 4
        self.junctions_max_split_length = 500000
        self.junctions_refinement_step_size = 2
        self.junctions_min_split_size = 15
        self.junctions_matches_threshold = 75
        self.junctions_coverage = 2

        # pair alignment parameter
        self.pairing_quality_threshold = None
        self.pairing_max_decoded_matches = 20
        self.pairing_min_decoded_strata = 1
        self.pairing_min_insert_size = 0
        self.pairing_max_insert_size = None
        self.pairing_max_edit_distance = 0.30
        self.pairing_min_matched_bases = 0.80
        self.pairing_max_extendable_matches = 0
        self.pairing_max_matches_per_extension = 0

    def update(self, configuration):
        """Update configuration from given map

        configuration -- the input configuration
        """
        for k, v in configuration.items():
            try:
                if v is not None:
                    setattr(self, k, v)
            except AttributeError:
                pass

    def __update_dict(self, target, source):
        if source is None:
            return
        for k, v in source.items():
            if v is not None:
                target[k] = v

    def map(self, name, configuration=None, dependencies=None, final=False, description=""):
        """Add mapping step"""
        step = MapStep(name, final=final, dependencies=dependencies, description=description)
        config = dotdict()

        config.index = self.index
        config.mismatches = self.genome_mismatches
        config.quality_threshold = self.genome_quality_threshold
        config.max_decoded_matches = self.genome_max_decoded_matches
        config.min_decoded_strata = self.genome_min_decoded_strata
        config.min_matched_bases = self.genome_min_matched_bases
        config.max_big_indel_length = self.genome_max_big_indel_length
        config.max_edit_distance = self.genome_max_edit_distance
        config.mismatch_alphabet = self.genome_mismatch_alphabet
        config.strata_after_best = self.genome_strata_after_best
        config.trim = None

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def pair(self, name, configuration=None, dependencies=None, final=False, description=""):
        """Add mapping step"""
        step = PairalignStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()

        config.index = self.index
        config.quality_threshold = self.pairing_quality_threshold
        config.max_decoded_matches = self.pairing_max_decoded_matches
        config.min_decoded_strata = self.pairing_min_decoded_strata
        config.min_insert_size = self.pairing_min_insert_size
        config.max_insert_size = self.pairing_max_insert_size
        config.max_edit_distance = self.pairing_max_edit_distance
        config.min_matched_bases = self.pairing_min_matched_bases
        config.max_extendable_matches = self.pairing_max_extendable_matches
        config.max_matches_per_extension = self.pairing_max_matches_per_extension

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def transcripts_annotation(self, name=None, configuration=None, dependencies=None, final=False, description=""):
        """Create annotation based transcriptom and map"""
        if self.annotation is None:
            logging.gemtools.info("No annotation specified, skipping annotation mapping")
            return -1
        step = TranscriptMapStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()
        config.denovo = False
        config.annotation = self.annotation
        config.index = self.transcript_index
        config.keys = self.transcript_keys
        config.mismatches = self.transcript_mismatches
        config.quality_threshold = self.transcript_quality_threshold
        config.max_decoded_matches = self.transcript_max_decoded_matches
        config.min_decoded_strata = self.transcript_min_decoded_strata
        config.min_matched_bases = self.transcript_min_matched_bases
        config.max_big_indel_length = self.transcript_max_big_indel_length
        config.max_edit_distance = self.transcript_max_edit_distance
        config.mismatch_alphabet = self.transcript_mismatch_alphabet
        config.strata_after_best = self.transcript_strata_after_best
        config.trim = None

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def transcripts_denovo(self, name=None, configuration=None, dependencies=None, final=False, description=""):
        """Create annotation based transcriptom and map"""
        step = TranscriptMapStep(name, dependencies=dependencies, final=final, description=description)

        config = dotdict()
        config.denovo = True
        config.index = self.denovo_index
        config.keys = self.denovo_keys
        config.mismatches = self.transcript_mismatches
        config.quality_threshold = self.transcript_quality_threshold
        config.max_decoded_matches = self.transcript_max_decoded_matches
        config.min_decoded_strata = self.transcript_min_decoded_strata
        config.min_matched_bases = self.transcript_min_matched_bases
        config.max_big_indel_length = self.transcript_max_big_indel_length
        config.max_edit_distance = self.transcript_max_edit_distance
        config.mismatch_alphabet = self.transcript_mismatch_alphabet
        config.strata_after_best = self.transcript_strata_after_best
        config.trim = None

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def create_transcriptome(self, name, configuration=None, dependencies=None, final=False, description="Create denovo transcript index"):
        step = CreateDenovoTranscriptomeStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()

        config.index = self.index
        config.mismatches = self.junction_mismatches
        config.max_junction_matches = self.junctions_max_junction_matches
        config.min_split_length = self.junctions_min_split_length
        config.max_split_length = self.junctions_max_split_length
        config.refinement_step_size = self.junctions_refinement_step_size
        config.min_split_size = self.junctions_min_split_size
        config.matches_threshold = self.junctions_matches_threshold
        config.coverage = self.junctions_coverage

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def bam(self, name, configuration=None, dependencies=None, final=False, description="Create BAM file"):
        step = CreateBamStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()

        config.index = self.index
        config.mapq = self.bam_mapq
        config.sort = self.bam_sort

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def prepare_input(self, name, configuration=None, dependencies=None, final=False, description="Prepare input"):
        step = PrepareInputStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def index_bam(self, name, configuration=None, dependencies=None, final=False, description="Index BAM file"):
        step = IndexBamStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def merge(self, name, configuration=None, dependencies=None, final=False, description="Merge alignments"):
        step = MergeStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()

        config.same_content = True
        config.index = self.index

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def merge_and_pair(self, name, configuration=None, dependencies=None, final=False, description="Merge and Pair alignments"):
        step = MergeAndPairStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()

        config.same_content = True
        config.index = self.index
        config.quality_threshold = self.pairing_quality_threshold
        config.max_decoded_matches = self.pairing_max_decoded_matches
        config.min_decoded_strata = self.pairing_min_decoded_strata
        config.min_insert_size = self.pairing_min_insert_size
        config.max_insert_size = self.pairing_max_insert_size
        config.max_edit_distance = self.pairing_max_edit_distance
        config.min_matched_bases = self.pairing_min_matched_bases
        config.max_extendable_matches = self.pairing_max_extendable_matches
        config.max_matches_per_extension = self.pairing_max_matches_per_extension


        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def open_input(self):
        """Open the original input files"""
        if len(self.input) == 1:
            return gem.files.open(self.input[0])
        else:
            return gem.filter.interleave([gem.files.open(f) for f in self.input], threads=max(1, self.threads / 2))


    def open_step(self, id, raw=False):
        """Open the original input files"""
        return self.steps[id].open(raw=raw)

    def initialize(self, silent=False):
        # check general parameter
        errors = []
        if self.input is None:
            errors.append("No input file specified")
        else:
            if len(self.input) == 1 and not self.single_end:
                # search for second file
                (n, p) = gem.utils.find_pair(self.input[0])
                if p is None:
                    #errors.append("Unable to deduce second pair input file from %s " % self.input[0])
                    logging.gemtools.warning("No second input file specified, assuming interleaved paird end reads!")
                else:
                    logging.gemtools.warning("Second pair input file found: %s " % p)
                    if self.name is None:
                        self.name = n
                    self.input.append(p)

            # check file counts
            if self.single_end and len(self.input) != 1:
                errors.append("Specify exactly one input file in single end mode")
            elif not self.single_end and len(self.input) > 2:
                errors.append("Paired end mode takes up to 2 input files, you specified %d" % (len(self.input)))
            else:
                # check input files
                input_abs = []
                for f in self.input:
                    if f is None or not os.path.exists(f):
                        errors.append("Input file not found: %s" % (f))
                    else:
                        # make aboslute path
                        input_abs.append(os.path.abspath(f))
                self.input = input_abs

        if self.name is None and self.input is not None and len(self.input) > 0:
            # get name from input files
            name = os.path.basename(self.input[0])
            if name.endswith(".gz"):
                name = name[-3:]
            name = name[:name.rfind(".")]

        if self.index is None:
            errors.append("No index specified")
        else:
            if not os.path.exists(self.index):
                errors.append("Index not found: %s" % (self.index))
            else:
                self.index = os.path.abspath(self.index)

        if self.output_dir is None:
            self.output_dir = os.getcwd()
        self.output_dir = os.path.abspath(self.output_dir)

        if self.annotation is not None:
            if not os.path.exists(self.annotation):
                errors.append("Annotaiton not found : %s" % self.annotation)
            else:
                self.annotation = os.path.abspath(self.annotation)

        if self.threads <= 0:
            self.threads = 1

        if self.transcript_index is None and self.annotation is not None:
            # guess the transcript index
            self.transcript_index = self.annotation + ".gem"
            if not os.path.exists(self.transcript_index):
                errors.append("Deduced transcript index not found: %s" % (self.transcript_index))
                errors.append("""We look for the transcriptome index just next to your annotation, but
could not find it there. Try to specify a path to the transcriptome index using
[-r|--transcript-index] <index>, where index is the path to the transcriptome
index generated from your annotation.""")
            else:
                self.transcript_index = os.path.abspath(self.transcript_index)
        elif self.annotation is not None and not os.path.exists(self.transcript_index):
            errors.append("Transcript index not found : %s")
        elif self.transcript_index is not None and os.path.exists(self.transcript_index):
            self.transcript_index = os.path.abspath(self.transcript_index)

        if self.transcript_keys is None and self.transcript_index is not None:
            self.transcript_keys = self.transcript_index[:-4] + ".junctions.keys"
            if not os.path.exists(self.transcript_keys):
                errors.append("Deduced transcript keys not found: %s" % (self.transcript_keys))
            else:
                self.transcript_keys = os.path.abspath(self.transcript_keys)
        elif self.transcript_keys is not None and not os.path.exists(self.transcript_keys):
            errors.append("Transcript keys not found : %s")
        elif self.transcript_keys is not None and os.path.exists(self.transcript_keys):
            self.transcript_keys = os.path.abspath(self.transcript_keys)

        if self.quality is None or str(self.quality) not in ["33", "64", "ignore", "offset-33", "offset-64"]:
            errors.append("Unknown quality offset: %s, please use 33, 64 or ignore" % (str(self.quality)))

        # check inpuf compression
        if self.compress_all and not self.direct_input:
            logging.gemtools.warning("Enabeling direct input for compressed temporay files")
            self.direct_input = True

        # annotaiton junctons should be generated if not found
        #self.junctions_annotation = None  # file with the annotation junctions

        # todo : can we check for a valid scoring scheme ?
        #self.scoring_scheme = "+U,+u,-s,-t,+1,-i,-a"  # scoring scheme
        if self.filter is not None and isinstance(self.filter, basestring):
            try:
                (a, b, c) = [int(s.strip()) for s in self.filter.split(",")]
                self.filter = (a, b, c)
            except Exception:
                errors.append("Invalid filter : %s" % (self.filter))

        if self.bam_mapq > 254:
            errors.append("Invalid mapq filter: %s" % (self.bam_mapq))

        # transcript mapping parameter
        if self.transcript_mismatches is None:
            self.transcript_mismatches = self.genome_mismatches
        if self.transcript_quality_threshold is None:
            self.transcript_quality_threshold = self.genome_quality_threshold

        self.transcript_min_decoded_strata = self.genome_min_decoded_strata
        self.transcript_min_matched_bases = self.genome_min_matched_bases
        self.transcript_max_big_indel_length = self.genome_max_big_indel_length
        self.transcript_max_edit_distance = self.genome_max_edit_distance

        if self.transcript_mismatch_alphabet is None:
            self.transcript_mismatch_alphabet = self.genome_mismatch_alphabet
        if self.transcript_strata_after_best is None:
            self.transcript_strata_after_best = self.genome_strata_after_best

        # pair alignment parameter
        if self.pairing_quality_threshold is None:
            self.pairing_quality_threshold = self.genome_quality_threshold

        if self.pairing_max_insert_size is None:
            self.pairing_max_insert_size = self.junctions_max_split_length

        if not self.single_end and len(errors) == 0:
            # check pairing information
            p1 = None
            p2 = None
            c = 0
            inp = self.open_input()
            for template in inp:
                if c == 0:
                    p1 = template.get_pair()
                elif c == 1:
                    p2 = template.get_pair()
                c += 1
                if c >= 2:
                    inp.close()
                    break
            if p1 == 0 or p2 == 0 or (p1 == 1 and p2 != 2) or (p2 == 1 and p1 != 2):
                errors.append("""Unable to get pairing information from input.
                    Please check your read id's and make sure its either in casava >= 1.8 format or the
                    ids end with /1 and /2""")

        if not silent and len(errors) > 0:
            raise PipelineError("Failed to initialize neccessary parameters:\n\n%s" % ("\n".join(errors)))

    def log_parameter(self):
        """Print selected parameters"""
        printer = logging.gemtools.gt
        run_step = len(self.run_steps) > 0

        printer("------------ Input Parameter ------------")
        printer("Input File(s)    : %s", self.input)
        printer("Index            : %s", self.index)
        printer("Annotation       : %s", self.annotation)
        printer("Transcript Index : %s", self.transcript_index)
        printer("Max read length  : %s", self.max_read_length)
        printer("")
        printer("Compress output  : %s", self.compress)
        printer("Compress all     : %s", self.compress_all)
        printer("Create BAM       : %s", self.bam_create)
        printer("Sort BAM         : %s", self.bam_sort)
        printer("Index BAM        : %s", self.bam_index)
        printer("Keep Temporary   : %s", not self.remove_temp)
        printer("")

        if not run_step:
            printer("------------ Pipeline Steps  ------------")
            for i, s in enumerate(self.steps):
                printer("%-2d - %20s : %s", i, s.name, s.description)
        else:
            printer("------------ Single Step execution  ------------")

        for i, s in enumerate(self.steps):
            if run_step and s.id not in self.run_steps:
                continue
            printer("")
            if len(s.dependencies) > 0:
                printer("------------ [ID:{0:-3} -- '{1}'] [Depends On: {2}] ------------".format(i, s.name, ", ".join([self.steps[j].name for j in s.dependencies])))
            else:
                printer("------------ [ID:{0:-3} -- '{1}'] ------------".format(i, s.name))

            for k, v in s.configuration.items():
                printer("%25s : %s", k, str(v))

            for i, f in enumerate(s.files()):
                if i == 0:
                    printer("%25s : %s", "Temporary Outputs", not s.final)
                    printer("%25s : %s", "Outputs", f)
                else:
                    printer("%25s : %s", "", f)
        printer("--------------------------------------------------------------")
        printer("")

    def load(self, file):
        """Load pipeline configuration from file"""
        if file is None or not os.path.exists(file):
            raise PipelineError("Configuration file not found: %s" % file)
        fd = open(file, "r")
        logging.gemtools.info("Loading configuraiton from %s", file)
        data = json.load(fd)
        for k, v in data.items():
            if hasattr(self, k):
                setattr(self, k, v)
        fd.close()

    def write_pipeline(self):
        """Write the pipeline and its configuration to a file
        based on the name
        """

        json_container = dict(self.__dict__)
        # skip the steps here, we convert them manually
        del json_container["steps"]
        del json_container["run_steps"]
        # remove non default values
        default_pipeline = MappingPipeline()
        default_pipeline.initialize(silent=True)
        for k, v in json_container.items():
            if hasattr(default_pipeline, k) and getattr(default_pipeline, k) == v:
                del json_container[k]

        # json_steps = []
        # for step in self.steps:
        #     s = {
        #         "id": step.id,
        #         "name": step.name,
        #         "description": step.description,
        #         "dependencies": step.dependencies,
        #         "configuration": step.configuration,
        #         "files": step._files
        #     }
        #     json_steps.append(s)

        # json_container['piepline_steps'] = json_steps
        file_name = "%s.gemtools" % (self.name)
        logging.gemtools.info("Writing Pipeline to %s", file_name)
        fd = open(file_name, "w")
        json.dump(json_container, fd, indent=2, sort_keys=True)
        fd.close()

    def run(self):
        run_step = len(self.run_steps) > 0

        if self.write_config and not run_step:
            self.write_pipeline()
        if self.dry:
            return

        error = False

        all_done = True
        final_files = []
        # check final steps if we are not running a set of steps
        if not run_step and not self.force:
            for step in self.steps:
                if step.final:
                    final_files.extend(step.files())
                    all_done = all_done & step.is_done()
            if all_done:
                logging.gemtools.warning("The following files already exist. Nothing to be run!\n\n%s\n" % ("\n".join(final_files)))
                return

        time = Timer()
        times = {}

        if run_step:
            # sort by id
            self.run_steps.sort()
        ids = [s.id for s in self.steps]
        if run_step:
            ids = self.run_steps

        for step_id in ids:
            step = self.steps[step_id]

            if run_step:
                # check dependencies are done
                for d in step.dependencies:
                    if not self.steps[d].is_done():
                        logging.gemtools.error("Step dependency is not completed : %s", self.steps[d].name)
                        error = True
                        break
            if run_step or self.force or not step.is_done():
                logging.gemtools.gt("Running step: %s" % step.name)
                t = Timer()
                try:
                    step.run()
                except KeyboardInterrupt:
                    logging.gemtools.warning("Job step canceled, forcing cleanup!")
                    error = True
                    step.cleanup(force=True)
                    break
                except PipelineError, e:
                    logging.gemtools.error("Error while executing step %s : %s" % (step.name, str(e)))
                    logging.gemtools.warning("Cleaning up after failed step : %s", step.name)
                    step.cleanup(force=True)
                    error = True
                    break
                except Exception, e:
                    traceback.print_exc()
                    logging.gemtools.error("Error while executing step %s : %s" % (step.name, str(e)))
                    logging.gemtools.warning("Cleaning up after failed step : %s", step.name)
                    step.cleanup(force=True)
                    error = True
                    break
                finally:
                    t.stop(step.name + " completed in %s", loglevel=None)
                    times[step.id] = t.end
                    logging.gemtools.gt("Step %s finished in : %s", step.name, t.end)
            else:
                logging.gemtools.warning("Skipping step %s, output already exists" % (step.name))

        # do celanup if not in error state
        if not error:
            logging.gemtools.debug("Cleanup after run")
            for step in self.steps:
                step.cleanup()

        time.stop("Completed in %s", loglevel=None)

        logging.gemtools.gt("Step Times")
        logging.gemtools.gt("-------------------------------------")
        for s in self.steps:
            if s.id in times:
                logging.gemtools.gt("{0:>25} : {1}".format(s.name, times[s.id]))
            else:
                logging.gemtools.gt("{0:>25} : skipped".format(s.name))
        logging.gemtools.gt("-------------------------------------")
        logging.gemtools.gt("Pipeline run finshed in %s", time.end)

    def cleanup(self):
        """Delete all remaining temporary and intermediate files
        """
        pass

    def create_file_name(self, suffix, file_suffix="map", final=False):
        """Create a result file name"""
        file = ""
        if final:
            suffix = None

        if suffix is not None and len(suffix) > 0:
            file = "%s/%s_%s.%s" % (self.output_dir, self.name, suffix, file_suffix)
        else:
            file = "%s/%s.%s" % (self.output_dir, self.name, file_suffix)
        if (self.compress_all or final and self.compress) and file_suffix in ["map", "fastq"]:
            file += ".gz"
        return file

    def gtf_junctions(self):
        """check if there is a .junctions file for the given annotation, if not,
        create it. Returns a tuple of the set of junctions and the output file.
        """
        if self.annotation is None:
            return (None, None)

        timer = Timer()
        gtf_junctions = self.annotation + ".junctions"
        out = None
        junctions = None
        if os.path.exists(gtf_junctions):
            logging.gemtools.info("Loading existing junctions from %s" % (gtf_junctions))
            out = gtf_junctions
            junctions = set(gem.junctions.from_junctions(gtf_junctions))
        else:
            out = self.create_file_name("gtf", file_suffix="junctions")
            if os.path.exists(out):
                logging.gemtools.info("Loading existing junctions from %s" % (out))
                junctions = set(gem.junctions.from_junctions(out))
            else:
                logging.gemtools.info("Extracting junctions from %s" % (self.annotation))
                junctions = set(gem.junctions.from_gtf(self.annotation))
                gem.junctions.write_junctions(junctions, out, self.index)

        logging.gemtools.info("%d Junctions from GTF" % (len(junctions)))
        timer.stop("GTF-Junctions prepared in %s")
        return (junctions, out)


